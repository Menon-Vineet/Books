{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Menon-Vineet/Books/blob/main/IMS_Bearing_Anomaly_Colab_Seamless.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40a0af1",
      "metadata": {
        "id": "a40a0af1"
      },
      "source": [
        "# NASA IMS Bearing Anomaly Detection — Colab-Seamless Notebook\n",
        "\n",
        "**Author:** Vineet Menon (el jefe)  \n",
        "**Date:** 2026-01-20\n",
        "\n",
        "This notebook is designed to run **seamlessly on Google Colab (Python 3.12)**.\n",
        "\n",
        "What I do here:\n",
        "- Download and parse the **NASA IMS Bearings** dataset (run-to-failure vibration snapshots)\n",
        "- Convert each 1-second vibration file into **compact statistical features**\n",
        "- Train and compare **classical unsupervised anomaly models** (scikit-learn)\n",
        "- Train a **BiLSTM Autoencoder** (PyTorch) to produce reconstruction-error anomaly scores\n",
        "\n",
        "Why no PyCaret?\n",
        "- PyCaret’s Python 3.12 support is still inconsistent across environments (especially Colab).\n",
        "- I implement the same core models directly in scikit-learn so the notebook remains reliable and shareable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7314269",
      "metadata": {
        "id": "b7314269"
      },
      "source": [
        "## 0. Install dependencies (run once)\n",
        "\n",
        "**Colab tip:** After running the install cell, use **Runtime → Disconnect and delete runtime**, reconnect, then run the notebook from the top.\n",
        "This avoids “half-loaded” binary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd13c21c",
      "metadata": {
        "id": "bd13c21c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install --no-cache-dir numpy==1.26.4 pandas==2.1.4\n",
        "!pip install --no-cache-dir scipy==1.11.4 scikit-learn==1.4.2\n",
        "!pip install --no-cache-dir matplotlib tqdm joblib\n",
        "!pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf42942",
      "metadata": {
        "id": "4bf42942"
      },
      "source": [
        "## 1. Imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6755841",
      "metadata": {
        "id": "e6755841"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "SEED: int = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "PROJECT_DIR = Path.cwd() / \"ims_anomaly_project\"\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "PLOTS_DIR = PROJECT_DIR / \"plots\"\n",
        "ARTIFACTS_DIR = PROJECT_DIR / \"artifacts\"\n",
        "\n",
        "for d in (DATA_DIR, PLOTS_DIR, ARTIFACTS_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project directory:\", PROJECT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd696be",
      "metadata": {
        "id": "bdd696be"
      },
      "source": [
        "## 2. Dataset utilities (download, parse, featurize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdd50fd",
      "metadata": {
        "id": "5cdd50fd"
      },
      "outputs": [],
      "source": [
        "NASA_BEARINGS_ZIP_URL: str = \"https://phm-datasets.s3.amazonaws.com/NASA/4.+Bearings.zip\"\n",
        "\n",
        "def download_file(url: str, dst: Path, overwrite: bool = False) -> Path:\n",
        "    \\\"\\\"\\\"Download a file to disk.\\\"\\\"\\\"\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dst.exists() and not overwrite:\n",
        "        return dst\n",
        "    print(f\"I am downloading: {url}\")\n",
        "    urllib.request.urlretrieve(url, dst)\n",
        "    return dst\n",
        "\n",
        "def unzip(zip_path: Path, extract_to: Path, overwrite: bool = False) -> Path:\n",
        "    \\\"\\\"\\\"Extract a ZIP archive.\\\"\\\"\\\"\n",
        "    if extract_to.exists() and overwrite:\n",
        "        shutil.rmtree(extract_to)\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(extract_to)\n",
        "    return extract_to\n",
        "\n",
        "def parse_ims_filename_to_datetime(filename: str) -> datetime:\n",
        "    \\\"\\\"\\\"Parse IMS timestamps from filenames like '2003.10.22.12.06.24'.\\\"\\\"\\\"\n",
        "    stem = Path(filename).stem\n",
        "    return datetime.strptime(stem, \"%Y.%m.%d.%H.%M.%S\")\n",
        "\n",
        "def find_test_folder(root: Path, test_name: str) -> Path:\n",
        "    \\\"\\\"\\\"Find test folder by name (e.g., '1st_test', '2nd_test', '3rd_test').\\\"\\\"\\\"\n",
        "    matches = list(root.rglob(test_name))\n",
        "    if not matches:\n",
        "        raise FileNotFoundError(f\"I could not find {test_name} under {root}\")\n",
        "    return matches[0]\n",
        "\n",
        "def load_ims_raw_signals(\n",
        "    test_folder: Path,\n",
        "    channel_col: int = 0,\n",
        "    max_files: Optional[int] = 800,\n",
        ") -> pd.DataFrame:\n",
        "    \\\"\\\"\\\"Load raw vibration snapshots into a DataFrame.\\\"\\\"\\\"\n",
        "    files = sorted([p for p in test_folder.iterdir() if p.is_file()])\n",
        "    if max_files is not None:\n",
        "        files = files[:max_files]\n",
        "\n",
        "    idx: List[pd.Timestamp] = []\n",
        "    sigs: List[np.ndarray] = []\n",
        "\n",
        "    for fp in tqdm(files, desc=f\"Loading raw signals (col={channel_col})\"):\n",
        "        ts = parse_ims_filename_to_datetime(fp.name)\n",
        "        arr = pd.read_csv(fp, header=None, delim_whitespace=True).iloc[:, channel_col].to_numpy(dtype=float)\n",
        "        idx.append(pd.Timestamp(ts))\n",
        "        sigs.append(arr)\n",
        "\n",
        "    df = pd.DataFrame({\"signal\": sigs}, index=pd.to_datetime(idx)).sort_index()\n",
        "    df.index.name = \"timestamp\"\n",
        "    return df\n",
        "\n",
        "def rms(x: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean(np.square(x), dtype=float)))\n",
        "\n",
        "def peak_to_peak(x: np.ndarray) -> float:\n",
        "    return float(np.max(x) - np.min(x))\n",
        "\n",
        "def featurize_snapshot(x: np.ndarray) -> Dict[str, float]:\n",
        "    \\\"\\\"\\\"Convert one raw snapshot into compact statistical features.\\\"\\\"\\\"\n",
        "    return {\n",
        "        \"rms\": rms(x),\n",
        "        \"mean\": float(np.mean(x)),\n",
        "        \"std\": float(np.std(x)),\n",
        "        \"p2p\": peak_to_peak(x),\n",
        "    }\n",
        "\n",
        "def featurize_timeseries(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    feats = [featurize_snapshot(sig) for sig in tqdm(raw_df[\"signal\"].to_list(), desc=\"Featurizing\")]\n",
        "    return pd.DataFrame(feats, index=raw_df.index)\n",
        "\n",
        "def plot_series(df: pd.DataFrame, columns: List[str], title: str, save_name: Optional[str] = None) -> None:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for c in columns:\n",
        "        plt.plot(df.index, df[c], label=c)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"timestamp\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    if save_name:\n",
        "        out = PLOTS_DIR / f\"{save_name}.png\"\n",
        "        plt.savefig(out, dpi=150)\n",
        "        print(\"Saved:\", out)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10418d16",
      "metadata": {
        "id": "10418d16"
      },
      "source": [
        "## 3. Download + load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecb7648",
      "metadata": {
        "id": "5ecb7648"
      },
      "outputs": [],
      "source": [
        "zip_path = DATA_DIR / \"NASA_4_Bearings.zip\"\n",
        "extract_root = DATA_DIR / \"NASA_4_Bearings\"\n",
        "\n",
        "download_file(NASA_BEARINGS_ZIP_URL, zip_path, overwrite=False)\n",
        "unzip(zip_path, extract_root, overwrite=False)\n",
        "\n",
        "TEST_NAME = \"2nd_test\"\n",
        "CHANNEL_COL = 0\n",
        "MAX_FILES = 800\n",
        "\n",
        "test_folder = find_test_folder(extract_root, TEST_NAME)\n",
        "print(\"Using test folder:\", test_folder)\n",
        "\n",
        "raw_df = load_ims_raw_signals(test_folder, channel_col=CHANNEL_COL, max_files=MAX_FILES)\n",
        "feat_df = featurize_timeseries(raw_df)\n",
        "\n",
        "print(\"Features shape:\", feat_df.shape)\n",
        "feat_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1405fd8",
      "metadata": {
        "id": "e1405fd8"
      },
      "source": [
        "## 4. Quick sanity plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98b63f0",
      "metadata": {
        "id": "c98b63f0"
      },
      "outputs": [],
      "source": [
        "plot_series(feat_df, [\"rms\"], title=f\"{TEST_NAME} - RMS over time\", save_name=\"rms_over_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1898cb0c",
      "metadata": {
        "id": "1898cb0c"
      },
      "source": [
        "## 5. Time-aware train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ccb9f4",
      "metadata": {
        "id": "25ccb9f4"
      },
      "outputs": [],
      "source": [
        "TRAIN_FRACTION = 0.6\n",
        "\n",
        "n = len(feat_df)\n",
        "cut = int(n * TRAIN_FRACTION)\n",
        "\n",
        "train_df = feat_df.iloc[:cut].copy()\n",
        "test_df = feat_df.iloc[cut:].copy()\n",
        "\n",
        "print(\"Train rows:\", len(train_df), \"| Test rows:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e907261",
      "metadata": {
        "id": "1e907261"
      },
      "source": [
        "## 6. Classical anomaly detection (scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5737616a",
      "metadata": {
        "id": "5737616a"
      },
      "outputs": [],
      "source": [
        "def fit_and_score_sklearn(\n",
        "    model_name: str,\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    score_percentile: float = 95.0,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Train an unsupervised model on train_df, then score test_df.\"\"\"\n",
        "    Xtr = train_df.to_numpy()\n",
        "    Xte = test_df.to_numpy()\n",
        "\n",
        "    if model_name == \"iforest\":\n",
        "        model = IsolationForest(n_estimators=400, random_state=SEED, contamination=\"auto\")\n",
        "        model.fit(Xtr)\n",
        "        scores = -model.score_samples(Xte)\n",
        "\n",
        "    elif model_name == \"lof\":\n",
        "        model = LocalOutlierFactor(n_neighbors=35, novelty=True)\n",
        "        model.fit(Xtr)\n",
        "        scores = -model.score_samples(Xte)\n",
        "\n",
        "    elif model_name == \"ocsvm\":\n",
        "        model = OneClassSVM(kernel=\"rbf\", nu=0.05, gamma=\"scale\")\n",
        "        model.fit(Xtr)\n",
        "        scores = -model.score_samples(Xte)\n",
        "\n",
        "    elif model_name == \"mcd\":\n",
        "        model = EllipticEnvelope(contamination=0.05, random_state=SEED)\n",
        "        model.fit(Xtr)\n",
        "        scores = -model.score_samples(Xte)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
        "\n",
        "    thresh = float(np.percentile(scores, score_percentile))\n",
        "    flags = (scores >= thresh).astype(int)\n",
        "\n",
        "    out = test_df.copy()\n",
        "    out[\"Anomaly_Score\"] = scores\n",
        "    out[\"Anomaly\"] = flags\n",
        "    return out\n",
        "\n",
        "MODELS = [\"iforest\", \"lof\", \"ocsvm\", \"mcd\"]\n",
        "results = {m: fit_and_score_sklearn(m, train_df, test_df) for m in MODELS}\n",
        "\n",
        "MODEL_TO_PLOT = \"iforest\"\n",
        "scored = results[MODEL_TO_PLOT].join(test_df[[\"rms\"]], how=\"left\")\n",
        "\n",
        "plot_series(scored, [\"rms\", \"Anomaly_Score\"], title=f\"{MODEL_TO_PLOT} - RMS vs Anomaly_Score\", save_name=f\"{MODEL_TO_PLOT}_score_vs_rms\")\n",
        "print(\"Anomaly rate:\", scored[\"Anomaly\"].mean() * 100, \"%\")\n",
        "scored.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4073c8",
      "metadata": {
        "id": "4f4073c8"
      },
      "source": [
        "## 7. BiLSTM Autoencoder (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d23da4",
      "metadata": {
        "id": "67d23da4"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class WindowConfig:\n",
        "    window: int = 32\n",
        "    stride: int = 1\n",
        "\n",
        "def make_windows(x: np.ndarray, cfg: WindowConfig) -> np.ndarray:\n",
        "    \"\"\"Create overlapping windows from (T, F) into (N, window, F).\"\"\"\n",
        "    T = x.shape[0]\n",
        "    if T < cfg.window:\n",
        "        raise ValueError(\"Not enough rows to build windows.\")\n",
        "    return np.stack([x[i:i+cfg.window] for i in range(0, T - cfg.window + 1, cfg.stride)], axis=0)\n",
        "\n",
        "class BiLSTMAutoencoder(nn.Module):\n",
        "    \"\"\"A compact BiLSTM autoencoder for sequence reconstruction.\"\"\"\n",
        "\n",
        "    def __init__(self, n_features: int, hidden: int = 32) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(n_features, hidden, batch_first=True, bidirectional=True)\n",
        "        self.decoder = nn.LSTM(2 * hidden, hidden, batch_first=True, bidirectional=True)\n",
        "        self.proj = nn.Linear(2 * hidden, n_features)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z, _ = self.encoder(x)\n",
        "        y, _ = self.decoder(z)\n",
        "        return self.proj(y)\n",
        "\n",
        "def train_autoencoder(model: nn.Module, x_train: np.ndarray, epochs: int = 8, lr: float = 1e-3, batch_size: int = 64) -> List[float]:\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device).train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    ds = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32))\n",
        "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    losses: List[float] = []\n",
        "    for ep in range(1, epochs + 1):\n",
        "        batch_losses = []\n",
        "        for (xb,) in dl:\n",
        "            xb = xb.to(device)\n",
        "            opt.zero_grad()\n",
        "            recon = model(xb)\n",
        "            loss = loss_fn(recon, xb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            batch_losses.append(float(loss.detach().cpu().item()))\n",
        "        mean_loss = float(np.mean(batch_losses))\n",
        "        losses.append(mean_loss)\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | loss={mean_loss:.6f}\")\n",
        "    return losses\n",
        "\n",
        "def reconstruction_scores(model: nn.Module, x: np.ndarray, batch_size: int = 128) -> np.ndarray:\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device).eval()\n",
        "    ds = torch.utils.data.TensorDataset(torch.tensor(x, dtype=torch.float32))\n",
        "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    scores: List[float] = []\n",
        "    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (xb,) in dl:\n",
        "            xb = xb.to(device)\n",
        "            recon = model(xb)\n",
        "            per_elem = loss_fn(recon, xb)\n",
        "            per_win = per_elem.mean(dim=(1, 2)).detach().cpu().numpy()\n",
        "            scores.extend(per_win.tolist())\n",
        "    return np.array(scores, dtype=float)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf7969b",
      "metadata": {
        "id": "dcf7969b"
      },
      "source": [
        "### 7.1 Train + score the BiLSTM AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c80aeb2",
      "metadata": {
        "id": "9c80aeb2"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "train_scaled = scaler.fit_transform(train_df)\n",
        "test_scaled = scaler.transform(test_df)\n",
        "\n",
        "cfg = WindowConfig(window=32, stride=1)\n",
        "\n",
        "train_w = make_windows(train_scaled, cfg)\n",
        "test_w = make_windows(test_scaled, cfg)\n",
        "\n",
        "print(\"Train windows:\", train_w.shape, \"| Test windows:\", test_w.shape)\n",
        "\n",
        "ae = BiLSTMAutoencoder(n_features=train_w.shape[-1], hidden=32)\n",
        "losses = train_autoencoder(ae, train_w, epochs=8, lr=1e-3, batch_size=64)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(losses)\n",
        "plt.title(\"BiLSTM AE training loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "train_scores = reconstruction_scores(ae, train_w)\n",
        "test_scores = reconstruction_scores(ae, test_w)\n",
        "\n",
        "THRESH_PCTL = 90\n",
        "thr = float(np.percentile(train_scores, THRESH_PCTL))\n",
        "flags = (test_scores >= thr).astype(int)\n",
        "print(f\"Threshold (p{THRESH_PCTL}) = {thr:.6f} | flagged={int(flags.sum())}/{len(flags)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ec139f",
      "metadata": {
        "id": "43ec139f"
      },
      "source": [
        "### 7.2 Align window scores to timestamps and plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a07b5cc",
      "metadata": {
        "id": "0a07b5cc"
      },
      "outputs": [],
      "source": [
        "aligned_index = test_df.index[cfg.window - 1:]\n",
        "ae_df = pd.DataFrame({\"Reconstruction_Score\": test_scores, \"Anomaly\": flags}, index=aligned_index)\n",
        "\n",
        "plot_df = ae_df.join(test_df[[\"rms\"]].iloc[cfg.window - 1:], how=\"left\")\n",
        "\n",
        "plot_series(plot_df, [\"rms\", \"Reconstruction_Score\"], title=\"BiLSTM AE - RMS vs Reconstruction Score\", save_name=\"bilstm_recon_vs_rms\")\n",
        "print(\"Sample anomalies:\")\n",
        "ae_df[ae_df[\"Anomaly\"] == 1].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486e055a",
      "metadata": {
        "id": "486e055a"
      },
      "source": [
        "## 8. Notes and next steps\n",
        "\n",
        "- Add more channels and richer features (spectral bands, envelope, crest factor)\n",
        "- Evaluate early-warning quality (lead time before failure)\n",
        "- Persist models/scaler for reuse (`joblib.dump`, `torch.save`)\n",
        "\n",
        "This notebook stays intentionally lightweight so it runs reliably in Colab.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}